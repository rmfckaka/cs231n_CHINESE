<html>
<head>
  <title>翻译3.2：最优化笔记（下）</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/305512 (zh-CN, DDL); Windows/10.0.15063 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1647"/>
<h1>翻译3.2：最优化笔记（下）</h1>

<div>
<span><div style="-evernote-webclip:true"><div><span><span style="font-family: inherit; font-size: 24px; font-style: inherit; font-variant: inherit; font-weight: 700; line-height: inherit; color: rgb(51, 51, 51);">原文如下</span><br/></span></div><div style="font-size: 16px; display: inline-block;"><div style="box-sizing:border-box;"><div style="box-sizing:inherit;overflow-x:hidden;font-family:-apple-system, &quot;Helvetica Neue&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif;font-weight:400;font-style:normal;text-rendering:optimizeLegibility;line-height:1;color:rgb(51, 51, 51);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;background:rgb(255, 255, 255);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;word-break:break-word;white-space:normal;margin:30px 0px;line-height:1.7;"><p style="box-sizing:inherit;margin:20px 0px;">内容列表：</p><ul style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">简介</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">损失函数可视化</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">最优化</li><ul style="box-sizing:inherit;padding:0px;padding-left:40px;margin:0px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">策略#1：随机搜索</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">策略#2：随机局部搜索</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">策略#3：跟随梯度</li></ul><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">梯度计算 <i style="box-sizing:inherit;"><b style="box-sizing:inherit;font-weight:700;">译者注：下篇起始处</b></i></li><ul style="box-sizing:inherit;padding:0px;padding-left:40px;margin:0px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">使用有限差值进行数值计算</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">微分分析计算梯度</li></ul><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">梯度下降</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">小结</li></ul><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">梯度计算</h2><p style="box-sizing:inherit;margin:20px 0px;">计算梯度有两种方法：一个是缓慢的近似方法（<b style="box-sizing:inherit;font-weight:700;">数值梯度法</b>），但实现相对简单。另一个方法（<b style="box-sizing:inherit;font-weight:700;">分析梯度法</b>）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：</p><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">利用有限差值计算梯度</b></p><p style="box-sizing:inherit;margin:20px 0px;">上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数<b style="box-sizing:inherit;font-weight:700;">f</b>和向量<b style="box-sizing:inherit;font-weight:700;">x，</b>计算<b style="box-sizing:inherit;font-weight:700;">f</b>的梯度的通用函数，它返回函数<b style="box-sizing:inherit;font-weight:700;">f</b>在点<b style="box-sizing:inherit;font-weight:700;">x处</b>的梯度：</p><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">def</span> <span style="box-sizing:inherit;font-weight:bold;color:rgb(153, 0, 0);">eval_numerical_gradient</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">f</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">一个f在x处的数值梯度法的简单实现</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- f是只有一个参数的函数</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- x是计算梯度的点</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">fx</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">f</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 在原点计算函数值</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">grad</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">zeros</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">shape</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">h</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0.00001</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 对x中所有的索引进行迭代</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">it</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">nditer</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">flags</span><span style="box-sizing:inherit;font-weight:bold;">=</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">'multi_index'</span><span style="box-sizing:inherit;">],</span> <span style="box-sizing:inherit;">op_flags</span><span style="box-sizing:inherit;font-weight:bold;">=</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">'readwrite'</span><span style="box-sizing:inherit;">])</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">while</span> <span style="box-sizing:inherit;font-weight:bold;">not</span> <span style="box-sizing:inherit;">it</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">finished</span><span style="box-sizing:inherit;">:</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 计算x+h处的函数值</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">ix</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">it</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">multi_index</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">old_value</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">ix</span><span style="box-sizing:inherit;">]</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">ix</span><span style="box-sizing:inherit;">]</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">old_value</span> <span style="box-sizing:inherit;font-weight:bold;">+</span> <span style="box-sizing:inherit;">h</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 增加h</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">fxh</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">f</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 计算f(x + h)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">ix</span><span style="box-sizing:inherit;">]</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">old_value</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 存到前一个值中 (非常重要)</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 计算偏导数</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">grad</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">ix</span><span style="box-sizing:inherit;">]</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">fxh</span> <span style="box-sizing:inherit;font-weight:bold;">-</span> <span style="box-sizing:inherit;">fx</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-weight:bold;">/</span> <span style="box-sizing:inherit;">h</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 坡度</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">it</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">iternext</span><span style="box-sizing:inherit;">()</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 到下个维度</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">return</span> <span style="box-sizing:inherit;">grad</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;">根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化h，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量<b style="box-sizing:inherit;font-weight:700;">grad</b>中。</p><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">实践考量</b>：注意在数学公式中，<strong style="box-sizing:inherit;">h</strong>的取值是趋近于0的，然而在实际中，用一个很小的数值（比如例子中的1e-5）就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的h。还有，实际中用<b style="box-sizing:inherit;font-weight:700;">中心差值公式（</b><strong style="box-sizing:inherit;">centered difference formula）</strong><img src="翻译3.2：最优化笔记（下）_files/equation.png" type="image/png" data-filename="equation.png" alt="[f(x+h)-f(x-h)]/2h" height="19" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="175"/>效果较好。细节可查看<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Numerical_differentiation" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">wiki</a>。</p><p style="box-sizing:inherit;margin:20px 0px;">可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：</p><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 要使用上面的代码我们需要一个只有一个参数的函数</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># (在这里参数就是权重)所以也包含了X_train和Y_train</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">def</span> <span style="box-sizing:inherit;font-weight:bold;color:rgb(153, 0, 0);">CIFAR10_loss_fun</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">return</span> <span style="box-sizing:inherit;">L</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">X_train</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">Y_train</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">)</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">W</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">random</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">rand</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">10</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">3073</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0.001</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 随机权重向量</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">df</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">eval_numerical_gradient</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">CIFAR10_loss_fun</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 得到梯度</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：</div><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">loss_original</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">CIFAR10_loss_fun</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 初始损失值</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">print</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">'original loss:</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%f</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">'</span> <span style="box-sizing:inherit;font-weight:bold;">%</span> <span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">loss_original</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">)</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 查看不同步长的效果</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">for</span> <span style="box-sizing:inherit;">step_size_log</span> <span style="box-sizing:inherit;font-weight:bold;">in</span> <span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">10</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">9</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">8</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">7</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">6</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">5</span><span style="box-sizing:inherit;">,</span><span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">4</span><span style="box-sizing:inherit;">,</span><span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">3</span><span style="box-sizing:inherit;">,</span><span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">2</span><span style="box-sizing:inherit;">,</span><span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">1</span><span style="box-sizing:inherit;">]:</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">step_size</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">10</span> <span style="box-sizing:inherit;font-weight:bold;">**</span> <span style="box-sizing:inherit;">step_size_log</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">W_new</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">W</span> <span style="box-sizing:inherit;font-weight:bold;">-</span> <span style="box-sizing:inherit;">step_size</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;">df</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 权重空间中的新位置</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">loss_new</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">CIFAR10_loss_fun</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">W_new</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">print</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">'for step size</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%f</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">new loss:</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%f</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">'</span> <span style="box-sizing:inherit;font-weight:bold;">%</span> <span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">step_size</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">loss_new</span><span style="box-sizing:inherit;">)</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 输出:</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># original loss: 2.200718</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-10 new loss: 2.200652</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-09 new loss: 2.200057</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-08 new loss: 2.194116</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-07 new loss: 2.135493</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-06 new loss: 1.647802</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-05 new loss: 2.844355</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-04 new loss: 25.558142</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-03 new loss: 254.086573</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-02 new loss: 2539.370888</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># for step size 1.000000e-01 new loss: 25392.214036</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">在梯度负方向上更新</strong>：在上面的代码中，为了计算<b style="box-sizing:inherit;font-weight:700;">W_new</b>，要注意我们是向着梯度<b style="box-sizing:inherit;font-weight:700;">df</b>的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">步长的影响</strong>：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作<i style="box-sizing:inherit;">学习率</i>）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><img src="翻译3.2：最优化笔记（下）_files/d8b52b9b9ca31e2132c436c39af2943c_b.jpg" type="image/jpeg" data-filename="d8b52b9b9ca31e2132c436c39af2943c_b.jpg" height="199" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="660"/><p style="box-sizing:inherit;margin:20px 0px;"></p><div>将步长效果视觉化的图例。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为<b style="box-sizing:inherit;font-weight:700;">学习率</b>）将会是我们在调参中最重要的超参数之一。</div><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">效率问题</strong>：你可能已经注意到，计算数值梯度的复杂性和参数的量线性相关。在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。</p><h3 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">微分分析计算梯度</h3><p style="box-sizing:inherit;margin:20px 0px;">使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于<i style="box-sizing:inherit;">h</i>值是选取了一个很小的数值，但真正的梯度定义中<i style="box-sizing:inherit;">h</i>趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做<strong style="box-sizing:inherit;">梯度检查</strong>。</p><p style="box-sizing:inherit;margin:20px 0px;">用SVM的损失函数在某个数据点上的计算来举例：</p><div><img src="翻译3.2：最优化笔记（下）_files/equation [1].png" type="image/png" data-filename="equation.png" alt="L_i=\displaystyle\sum_{j\not =y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)]" height="38" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="256"/></div><p style="box-sizing:inherit;margin:20px 0px;">可以对函数进行微分。比如，对<img src="翻译3.2：最优化笔记（下）_files/equation [2].png" type="image/png" data-filename="equation.png" alt="w_{y_i}" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="22"/>进行微分得到：</p><img src="翻译3.2：最优化笔记（下）_files/equation [3].png" type="image/png" data-filename="equation.png" alt="\displaystyle\nabla_{w_{y_i}}L_i=-(\sum_{j\not=y_i}1(w^T_jx_i-w^T_{y_i}x_i+\Delta>0))x_i" height="38" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="312"/><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;"><i style="box-sizing:inherit;">译者注：原公式中1为空心字体，尝试\mathbb{}等多种方法仍无法实现，请知友指点。</i></b></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>其中<img src="翻译3.2：最优化笔记（下）_files/equation [4].png" type="image/png" data-filename="equation.png" alt="1" height="13" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="6"/>是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以<img src="翻译3.2：最优化笔记（下）_files/equation [5].png" type="image/png" data-filename="equation.png" alt="x_i" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些<img src="翻译3.2：最优化笔记（下）_files/equation [6].png" type="image/png" data-filename="equation.png" alt="j\not =y_i" height="16" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="42"/>行的梯度是：</div><div><img src="翻译3.2：最优化笔记（下）_files/equation [7].png" type="image/png" data-filename="equation.png" alt="\displaystyle\nabla_{w_j}L_i=1(w^T_jx_i-w^T_{y_i}x_i+\Delta>0)x_i" height="21" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="254"/></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。</div><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">梯度下降</h2><p style="box-sizing:inherit;margin:20px 0px;"></p><div>现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为<i style="box-sizing:inherit;">梯度下降</i>，他的<strong style="box-sizing:inherit;">普通</strong>版本是这样的：</div><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 普通的梯度下降</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">while</span> <span style="box-sizing:inherit;color:rgb(153, 153, 153);">True</span><span style="box-sizing:inherit;">:</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">weights_grad</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">evaluate_gradient</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">loss_fun</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">data</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">weights</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">weights</span> <span style="box-sizing:inherit;font-weight:bold;">+=</span> <span style="box-sizing:inherit;font-weight:bold;">-</span> <span style="box-sizing:inherit;">step_size</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;">weights_grad</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 进行梯度更新</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;">这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">小批量数据梯度下降（</strong><strong style="box-sizing:inherit;">Mini-batch gradient descent</strong><strong style="box-sizing:inherit;">）</strong>：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的<b style="box-sizing:inherit;font-weight:700;">小批量（batches）</b>数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。这个小批量数据就用来实现一个参数更新：</p><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 普通的小批量数据梯度下降</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">while</span> <span style="box-sizing:inherit;color:rgb(153, 153, 153);">True</span><span style="box-sizing:inherit;">:</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">data_batch</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">sample_training_data</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">data</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">256</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 256个数据</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">weights_grad</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">evaluate_gradient</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">loss_fun</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">data_batch</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">weights</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">weights</span> <span style="box-sizing:inherit;font-weight:bold;">+=</span> <span style="box-sizing:inherit;font-weight:bold;">-</span> <span style="box-sizing:inherit;">step_size</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;">weights_grad</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 参数更新</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;">这个方法之所以效果不错，是因为训练集中的数据都是相关的。要理解这一点，可以想象一个极端情况：在ILSVRC中的120万个图像是1000张不同图片的复制（每个类别1张图片，每张图片有1200张复制）。那么显然计算这1200张复制图像的梯度就应该是一样的。对比120万张图片的数据损失的均值与只计算1000张的子集的数据损失均值时，结果应该是一样的。实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。</p><p style="box-sizing:inherit;margin:20px 0px;">小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<strong style="box-sizing:inherit;">随机梯度下降（</strong><strong style="box-sizing:inherit;">Stochastic Gradient Descent 简称SGD</strong><strong style="box-sizing:inherit;">）</strong>，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">小结</h2><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><img src="翻译3.2：最优化笔记（下）_files/03b3eccf18ee3760e219f9f95ec14305_b.png" type="image/png" data-filename="03b3eccf18ee3760e219f9f95ec14305_b.png" height="154" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="1572"/><p style="box-sizing:inherit;margin:20px 0px;">信息流的总结图例。数据集中的(x,y)是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量<b style="box-sizing:inherit;font-weight:700;">f</b>中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p><p style="box-sizing:inherit;margin:20px 0px;">—————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;">在本节课中：</p><ul style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">将损失函数比作了一个<b style="box-sizing:inherit;font-weight:700;">高维度的最优化地形</b>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">函数的<strong style="box-sizing:inherit;">梯度</strong>给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是<i style="box-sizing:inherit;">h</i>，用来计算数值梯度）。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">参数更新需要有技巧地设置<strong style="box-sizing:inherit;">步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用<strong style="box-sizing:inherit;">梯度检查</strong>来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">介绍了<strong style="box-sizing:inherit;">梯度下降</strong>算法，它在循环中迭代地计算梯度并更新参数。</li></ul><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">预告</strong>：这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。下节中，将介绍如何使用链式法则来高效地计算梯度，也就是通常所说的<b style="box-sizing:inherit;font-weight:700;">反向传播（</b><strong style="box-sizing:inherit;">backpropagation）机制</strong>。该机制能够对包含卷积神经网络在内的几乎所有类型的神经网络的损失函数进行高效的最优化。</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">最优化笔记全文翻译完</b>。</div></div></div></div></div></div></div></div></div><div><br/></div></div><div><br/></div><div><br/></div></span>
</div></body></html> 