<html>
<head>
  <title>翻译3.1：最优化笔记（上）</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/305512 (zh-CN, DDL); Windows/10.0.15063 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1604"/>
<h1>翻译3.1：最优化笔记（上）</h1>

<div>
<span><div style="-evernote-webclip:true"><div><span><span style="font-family: inherit; font-size: 24px; font-style: inherit; font-variant: inherit; font-weight: 700; line-height: inherit; color: rgb(51, 51, 51);">原文如下</span><br/></span></div><div style="font-size: 16px; display: inline-block;"><div style="box-sizing:border-box;"><div style="box-sizing:inherit;overflow-x:hidden;font-family:-apple-system, &quot;Helvetica Neue&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif;font-weight:400;font-style:normal;text-rendering:optimizeLegibility;line-height:1;color:rgb(51, 51, 51);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;background:rgb(255, 255, 255);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;word-break:break-word;white-space:normal;margin:30px 0px;line-height:1.7;"><p style="box-sizing:inherit;margin:20px 0px;">内容列表：</p><ul style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">简介</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">损失函数可视化</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">最优化</li><ul style="box-sizing:inherit;padding:0px;padding-left:40px;margin:0px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">策略#1：随机搜索</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">策略#2：随机局部搜索</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">策略#3：跟随梯度 <b style="box-sizing:inherit;font-weight:700;"><i style="box-sizing:inherit;">译者注：上篇截止处</i></b></li></ul><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">梯度计算</li><ul style="box-sizing:inherit;padding:0px;padding-left:40px;margin:0px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">使用有限差值进行数值计算</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">微分计算梯度</li></ul><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">梯度下降</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">小结</li></ul><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">简介</h2><p style="box-sizing:inherit;margin:20px 0px;">在上一节中，我们介绍了图像分类任务中的两个关键部分：</p><ol style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:decimal;list-style-position:outside;">基于参数的<b style="box-sizing:inherit;font-weight:700;">评分函数。</b>该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:decimal;list-style-position:outside;"><b style="box-sizing:inherit;font-weight:700;">损失函数</b>。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。</li></ol><p style="box-sizing:inherit;margin:20px 0px;"></p><div>上节中，线性函数的形式是<img src="翻译3.1：最优化笔记（上）_files/equation.png" type="image/png" data-filename="equation.png" alt="f(x_i, W)=Wx_i" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="114"/>，而SVM实现的公式是：</div><div><img src="翻译3.1：最优化笔记（上）_files/equation [1].png" type="image/png" data-filename="equation.png" alt="L=\displaystyle\frac{1}{N}\sum_i\sum_{j\not= y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)]+\alpha R(W)" height="46" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="434"/></div><p style="box-sizing:inherit;margin:20px 0px;">对于图像数据<img src="翻译3.1：最优化笔记（上）_files/equation [2].png" type="image/png" data-filename="equation.png" alt="x_i" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>，如果基于参数集<img src="翻译3.1：最优化笔记（上）_files/equation [3].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>做出的分类预测与真实情况比较一致，那么计算出来的损失值<img src="翻译3.1：最优化笔记（上）_files/equation [4].png" type="image/png" data-filename="equation.png" alt="L" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="11"/>就很低。现在介绍第三个，也是最后一个关键部分：<b style="box-sizing:inherit;font-weight:700;">最优化Optimization</b>。最优化是寻找能使得损失函数值最小化的参数<img src="翻译3.1：最优化笔记（上）_files/equation [5].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>的过程。</p><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">铺垫</b>：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。</p><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">损失函数可视化</h2><p style="box-sizing:inherit;margin:20px 0px;">本课中讨论的损失函数一般都是定义在高维度的空间中（比如，在CIFAR-10中一个线性分类器的权重矩阵大小是[10x3073]，就有30730个参数），这样要将其可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能得到一些直观感受。例如，随机生成一个权重矩阵<img src="翻译3.1：最优化笔记（上）_files/equation [6].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向<img src="翻译3.1：最优化笔记（上）_files/equation [7].png" type="image/png" data-filename="equation.png" alt="W_1" height="15" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="20"/>并且沿着此方向计算损失值，计算方法是根据不同的<img src="翻译3.1：最优化笔记（上）_files/equation [8].png" type="image/png" data-filename="equation.png" alt="a" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>值来计算<img src="翻译3.1：最优化笔记（上）_files/equation [9].png" type="image/png" data-filename="equation.png" alt="L(W+aW_1)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="90"/>。这个过程将生成一个图表，其x轴是<img src="翻译3.1：最优化笔记（上）_files/equation [10].png" type="image/png" data-filename="equation.png" alt="a" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>值，y轴是损失函数值。同样的方法还可以用在两个维度上，通过改变<img src="翻译3.1：最优化笔记（上）_files/equation [11].png" type="image/png" data-filename="equation.png" alt="a,b" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="23"/>来计算损失值<img src="翻译3.1：最优化笔记（上）_files/equation [12].png" type="image/png" data-filename="equation.png" alt="L(W+aW_1+bW_2)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="140"/>，从而给出二维的图像。在图像中，<img src="翻译3.1：最优化笔记（上）_files/equation [13].png" type="image/png" data-filename="equation.png" alt="a,b" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="23"/>可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示：</p><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"><img src="翻译3.1：最优化笔记（上）_files/94dd0714f65ef94b3cbfff4780b1988d_b.png" type="image/png" data-filename="94dd0714f65ef94b3cbfff4780b1988d_b.png" height="220" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="1586"/>一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。<b style="box-sizing:inherit;font-weight:700;">左</b>：a值变化在某个维度方向上对应的的损失值变化。<b style="box-sizing:inherit;font-weight:700;">中和右</b>：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。</p><p style="box-sizing:inherit;margin:20px 0px;">—————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div>我们可以通过数学公式来解释损失函数的分段线性结构。对于一个单独的数据，有损失函数的计算公式如下：</div><div><img src="翻译3.1：最优化笔记（上）_files/equation [14].png" type="image/png" data-filename="equation.png" alt="Li=\sum_{j\not=y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+1)]" height="38" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="251"/></div><p style="box-sizing:inherit;margin:20px 0px;">通过公式可见，每个样本的数据损失值是以<img src="翻译3.1：最优化笔记（上）_files/equation [15].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>为参数的线性函数的总和（零阈值来源于<img src="翻译3.1：最优化笔记（上）_files/equation [16].png" type="image/png" data-filename="equation.png" alt="max(0,-)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="72"/>函数）。<img src="翻译3.1：最优化笔记（上）_files/equation [17].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>的每一行（即<img src="翻译3.1：最优化笔记（上）_files/equation [18].png" type="image/png" data-filename="equation.png" alt="w_j" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="17"/>），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：</p><div><img src="翻译3.1：最优化笔记（上）_files/equation [19].png" type="image/png" data-filename="equation.png" alt="L_0=max(0,w^T_1x_0-w^T_0x_0+1)+max(0,w^T_2x_0-w^T_0x_0+1)" height="19" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="424"/></div><div><img src="翻译3.1：最优化笔记（上）_files/equation [20].png" type="image/png" data-filename="equation.png" alt="L_1=max(0,w^T_0x_1-w^T_1x_1+1)+max(0,w^T_2x_1-w^T_1x_1+1)" height="19" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="424"/></div><div><img src="翻译3.1：最优化笔记（上）_files/equation [21].png" type="image/png" data-filename="equation.png" alt="L_2=max(0,w^T_0x_2-w^T_2x_2+1)+max(0,w^T_1x_2-w^T_2x_2+1)" height="19" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="424"/></div><div><img src="翻译3.1：最优化笔记（上）_files/equation [22].png" type="image/png" data-filename="equation.png" alt="L=(L_0+L_1+L_2)/3" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="154"/></div><p style="box-sizing:inherit;margin:20px 0px;">因为这些例子都是一维的，所以数据<img src="翻译3.1：最优化笔记（上）_files/equation [23].png" type="image/png" data-filename="equation.png" alt="x_i" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>和权重<img src="翻译3.1：最优化笔记（上）_files/equation [24].png" type="image/png" data-filename="equation.png" alt="w_j" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="17"/>都是数字。观察<img src="翻译3.1：最优化笔记（上）_files/equation [25].png" type="image/png" data-filename="equation.png" alt="w_0" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>，可以看到上面的式子中一些项是<img src="翻译3.1：最优化笔记（上）_files/equation [26].png" type="image/png" data-filename="equation.png" alt="w_0" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下：——————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div><img src="翻译3.1：最优化笔记（上）_files/3f6fbcd487b1c214e8fea1ea66eb413e_b.png" type="image/png" data-filename="3f6fbcd487b1c214e8fea1ea66eb413e_b.png" height="149" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="1450"/>从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。</div><p style="box-sizing:inherit;margin:20px 0px;">——————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div>需要多说一句的是，你可能根据SVM的损失函数的碗状外观猜出它是一个<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Convex_function" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">凸函数</a>。关于如何高效地最小化凸函数的论文有很多，你也可以学习斯坦福大学关于（<a href="http://link.zhihu.com/?target=http%3A//stanford.edu/%7Eboyd/cvxbook/" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">凸函数最优化</a>）的课程。但是一旦我们将<img src="翻译3.1：最优化笔记（上）_files/equation [27].png" type="image/png" data-filename="equation.png" alt="f" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="10"/>函数扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。</div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><i style="box-sizing:inherit;">不可导的损失函数。</i>作为一个技术笔记，你要注意到：由于max操作，损失函数中存在一些<i style="box-sizing:inherit;">不可导点（kinks），</i>这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Subderivative" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">次梯度（subgradient）</a>依然存在且常常被使用。在本课中，我们将交换使用<i style="box-sizing:inherit;">次梯度</i>和<i style="box-sizing:inherit;">梯度</i>两个术语。</div><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">最优化 Optimization</h2><p style="box-sizing:inherit;margin:20px 0px;"></p><div>重申一下：损失函数可以量化某个具体权重集<b style="box-sizing:inherit;font-weight:700;">W</b>的质量。而最优化的目标就是找到能够最小化损失函数值的<b style="box-sizing:inherit;font-weight:700;">W</b> 。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM 损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。</div><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">策略#1：一个差劲的初始方案：随机搜索</b></p><p style="box-sizing:inherit;margin:20px 0px;"></p><div>既然确认参数集<b style="box-sizing:inherit;font-weight:700;">W</b>的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。过程如下：</div><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 假设函数L对损失函数进行评价</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">bestloss</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 128, 128);">float</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;inf&quot;</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># Python assigns the highest possible float value</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">for</span> <span style="box-sizing:inherit;">num</span> <span style="box-sizing:inherit;font-weight:bold;">in</span> <span style="box-sizing:inherit;color:rgb(0, 128, 128);">xrange</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">1000</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">W</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">random</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">randn</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">10</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">3073</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0.0001</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># generate random parameters</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">loss</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">L</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">X_train</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">Y_train</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># get the loss over the entire training set</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">if</span> <span style="box-sizing:inherit;">loss</span> <span style="box-sizing:inherit;font-weight:bold;">&lt;</span> <span style="box-sizing:inherit;">bestloss</span><span style="box-sizing:inherit;">:</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># keep track of the best solution</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">bestloss</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">loss</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">bestW</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">W</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">print</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">'in attempt</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%d</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">the loss was</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%f</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">, best</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%f</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">'</span> <span style="box-sizing:inherit;font-weight:bold;">%</span> <span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">num</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">loss</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">bestloss</span><span style="box-sizing:inherit;">)</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 输出:</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># in attempt 0 the loss was 9.401632, best 9.401632</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># in attempt 1 the loss was 8.959668, best 8.959668</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># in attempt 2 the loss was 9.044034, best 8.959668</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># in attempt 3 the loss was 9.278948, best 8.959668</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># in attempt 4 the loss was 8.857370, best 8.857370</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># in attempt 5 the loss was 8.943151, best 8.857370</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># in attempt 6 the loss was 8.605604, best 8.605604</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># ... (trunctated: continues for 1000 lines)</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>在上面的代码中，我们尝试了若干随机生成的权重矩阵<b style="box-sizing:inherit;font-weight:700;">W</b>，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重<b style="box-sizing:inherit;font-weight:700;">W</b>取出，然后去跑测试集：</div><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">scores</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">Wbest</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">dot</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">Xte_cols</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 10 x 10000, the class scores for all test examples</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 找到在每列中评分值最大的索引（即预测的分类）</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">Yte_predict</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">argmax</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">scores</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">axis</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 以及计算准确率</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">mean</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">Yte_predict</span> <span style="box-sizing:inherit;font-weight:bold;">==</span> <span style="box-sizing:inherit;">Yte</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 返回 0.1555</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>验证集上表现最好的权重<b style="box-sizing:inherit;font-weight:700;">W</b>跑测试集的准确率是<strong style="box-sizing:inherit;">15.5%，</strong>而完全随机猜的准确率是10%，如此看来，这个准确率对于这样一个不经过大脑的策略来说，还算不错嘛！</div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><strong style="box-sizing:inherit;">核心思路：迭代优化</strong>。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重<b style="box-sizing:inherit;font-weight:700;">W</b>非常困难，甚至是不可能的（尤其当<b style="box-sizing:inherit;font-weight:700;">W</b>中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集<b style="box-sizing:inherit;font-weight:700;">W</b>取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的<b style="box-sizing:inherit;font-weight:700;">W</b>开始，然后对其迭代取优，每次都让它的损失值变得更小一点。</div><blockquote style="box-sizing:inherit;padding-left:1.2em;margin:20px 0px;color:rgb(51, 51, 51);border-left:4px solid rgb(226, 227, 228);"><div>我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。</div></blockquote><p style="box-sizing:inherit;margin:20px 0px;"></p><div><strong style="box-sizing:inherit;">蒙眼徒步者的比喻</strong>：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为<b style="box-sizing:inherit;font-weight:700;">W</b>是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。</div><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">策略#2：随机本地搜索</b></p><p style="box-sizing:inherit;margin:20px 0px;"></p><div>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机<img src="翻译3.1：最优化笔记（上）_files/equation [28].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>开始，然后生成一个随机的扰动<img src="翻译3.1：最优化笔记（上）_files/equation [29].png" type="image/png" data-filename="equation.png" alt="\delta W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="26"/> ，只有当<img src="翻译3.1：最优化笔记（上）_files/equation [30].png" type="image/png" data-filename="equation.png" alt="W+\delta W" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="64"/>的损失值变低，我们才会更新。这个过程的具体代码如下：</div><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">W</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">random</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">randn</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">10</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">3073</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0.001</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># 生成随机初始W</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">bestloss</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 128, 128);">float</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;inf&quot;</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">for</span> <span style="box-sizing:inherit;">i</span> <span style="box-sizing:inherit;font-weight:bold;">in</span> <span style="box-sizing:inherit;color:rgb(0, 128, 128);">xrange</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">1000</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">step_size</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0.0001</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">Wtry</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">W</span> <span style="box-sizing:inherit;font-weight:bold;">+</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">random</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">randn</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">10</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">3073</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;">step_size</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">loss</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">L</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">Xtr_cols</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">Ytr</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">Wtry</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">if</span> <span style="box-sizing:inherit;">loss</span> <span style="box-sizing:inherit;font-weight:bold;">&lt;</span> <span style="box-sizing:inherit;">bestloss</span><span style="box-sizing:inherit;">:</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">W</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">Wtry</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">bestloss</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">loss</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">print</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">'iter</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%d</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">loss is</span> <span style="box-sizing:inherit;color:rgb(221, 51, 34);">%f</span><span style="box-sizing:inherit;color:rgb(221, 51, 34);">'</span> <span style="box-sizing:inherit;font-weight:bold;">%</span> <span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">i</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">bestloss</span><span style="box-sizing:inherit;">)</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>使用同样的数据（1000），这个方法可以得到<strong style="box-sizing:inherit;">21.4%</strong>的分类准确率。这个比策略一好，但是依然过于浪费计算资源。</div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">策略#3：跟随梯度</b></div><p style="box-sizing:inherit;margin:20px 0px;">前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的<b style="box-sizing:inherit;font-weight:700;">梯度（gradient）</b>。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。</p><p style="box-sizing:inherit;margin:20px 0px;">在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数<strong style="box-sizing:inherit;">derivatives</strong>）。对一维函数的求导公式如下：</p><div><img src="翻译3.1：最优化笔记（上）_files/equation [31].png" type="image/png" data-filename="equation.png" alt="\displaystyle\frac{df(x)}{dx}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}" height="37" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="204"/></div><p style="box-sizing:inherit;margin:20px 0px;">当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">最优化笔记（上）完。</b></p></div></div></div></div></div></div></div></div><div><br/></div></div><div><br/></div><div><br/></div><div><br/></div></span>
</div></body></html> 