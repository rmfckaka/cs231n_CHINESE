<html>
<head>
  <title>翻译4.1.1：神经网络笔记1（上）</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/305512 (zh-CN, DDL); Windows/10.0.15063 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1765"/>
<h1>翻译4.1.1：神经网络笔记1（上）</h1>

<div>
<span><div style="-evernote-webclip:true"><div><span><span style="font-family: inherit; font-size: 24px; font-style: inherit; font-variant: inherit; font-weight: 700; line-height: inherit; color: rgb(51, 51, 51);">原文如下</span><br/></span></div><div style="font-size: 16px; display: inline-block;"><div style="box-sizing:border-box;"><div style="box-sizing:inherit;overflow-x:hidden;font-family:-apple-system, &quot;Helvetica Neue&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif;font-weight:400;font-style:normal;text-rendering:optimizeLegibility;line-height:1;color:rgb(51, 51, 51);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;background:rgb(255, 255, 255);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;word-break:break-word;white-space:normal;margin:30px 0px;line-height:1.7;"><p style="box-sizing:inherit;margin:20px 0px;">内容列表：</p><ul style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">不用大脑做类比的快速简介</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">单个神经元建模</li><ul style="box-sizing:inherit;padding:0px;padding-left:40px;margin:0px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">生物动机和连接</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">作为线性分类器的单个神经元</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">常用的激活函数 <b style="box-sizing:inherit;font-weight:700;"><i style="box-sizing:inherit;">译者注：上篇翻译截止处</i></b></li></ul><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">神经网络结构</li><ul style="box-sizing:inherit;padding:0px;padding-left:40px;margin:0px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">层组织</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">前向传播计算例子</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">表达能力</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">设置层的数量和尺寸</li></ul><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">小节</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">参考文献</li></ul><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">快速简介</h2><p style="box-sizing:inherit;margin:20px 0px;">在不诉诸大脑的类比的情况下，依然是可以对神经网络算法进行介绍的。在线性分类一节中，在给出图像的情况下，是使用<img src="翻译4.1.1：神经网络笔记1（上）_files/equation.png" type="image/png" data-filename="equation.png" alt="s=Wx" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="57"/>来计算不同视觉类别的评分，其中<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [1].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>是一个矩阵，<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [2].png" type="image/png" data-filename="equation.png" alt="x" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>是一个输入列向量，它包含了图像的全部像素数据。在使用数据库CIFAR-10的案例中，<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [3].png" type="image/png" data-filename="equation.png" alt="x" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>是一个[3072x1]的列向量，<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [4].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>是一个[10x3072]的矩阵，所以输出的评分是一个包含10个分类评分的向量。</p><p style="box-sizing:inherit;margin:20px 0px;">神经网络算法则不同，它的计算公式是<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [5].png" type="image/png" data-filename="equation.png" alt="s=W_2max(0,W_1x)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="141"/>。其中<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [6].png" type="image/png" data-filename="equation.png" alt="W_1" height="15" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="20"/>的含义是这样的：举个例子来说，它可以是一个[100x3072]的矩阵，其作用是将图像转化为一个100维的过渡向量。函数<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [7].png" type="image/png" data-filename="equation.png" alt="max(0,-)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="72"/>是非线性的，它会作用到每个元素。这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。最终，矩阵<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [8].png" type="image/png" data-filename="equation.png" alt="W_2" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="21"/>的尺寸是[10x100]，因此将得到10个数字，这10个数字可以解释为是分类的评分。注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是<i style="box-sizing:inherit;">改变</i>的关键点。参数<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [9].png" type="image/png" data-filename="equation.png" alt="W_1,W_2" height="15" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="50"/>将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。</p><p style="box-sizing:inherit;margin:20px 0px;">一个三层的神经网络可以类比地看做<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [10].png" type="image/png" data-filename="equation.png" alt="s=W_3max(0,W_2max(0,W_1x))" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="223"/>，其中<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [11].png" type="image/png" data-filename="equation.png" alt="W_1,W_2,W_3" height="15" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="80"/>是需要进行学习的参数。中间隐层的尺寸是网络的超参数，后续将学习如何设置它们。现在让我们先从神经元或者网络的角度理解上述计算。</p><h1 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">单个神经元建模</h1><p style="box-sizing:inherit;margin:20px 0px;"></p><div>神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，因为神经网络毕竟是从这里得到了启发。</div><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">生物动机与连接</h2><p style="box-sizing:inherit;margin:20px 0px;">大脑的基本计算单位是<b style="box-sizing:inherit;font-weight:700;">神经元（</b><strong style="box-sizing:inherit;">neuron</strong><b style="box-sizing:inherit;font-weight:700;">）</b>。人类的神经系统中大约有860亿个神经元，它们被大约10^14-10^15个<b style="box-sizing:inherit;font-weight:700;">突触（</b><strong style="box-sizing:inherit;">synapses</strong><b style="box-sizing:inherit;font-weight:700;">）</b>连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的<b style="box-sizing:inherit;font-weight:700;">树突</b>获得输入信号，然后沿着它唯一的<b style="box-sizing:inherit;font-weight:700;">轴突（</b><strong style="box-sizing:inherit;">axon</strong><b style="box-sizing:inherit;font-weight:700;">）</b>产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。</p><p style="box-sizing:inherit;margin:20px 0px;">在神经元的计算模型中，沿着轴突传播的信号（比如<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [12].png" type="image/png" data-filename="equation.png" alt="x_0" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="15"/>）将基于突触的突触强度（比如<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [13].png" type="image/png" data-filename="equation.png" alt="w_0" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>），与其他神经元的树突进行乘法交互（比如<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [14].png" type="image/png" data-filename="equation.png" alt="w_0x_0" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="33"/>）。其观点是，突触的强度（也就是权重<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [15].png" type="image/png" data-filename="equation.png" alt="w" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="12"/>），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会<i style="box-sizing:inherit;">激活</i>，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个<i style="box-sizing:inherit;">速率编码</i>的观点，将神经元的激活率建模为<b style="box-sizing:inherit;font-weight:700;">激活函数（</b><strong style="box-sizing:inherit;">activation function</strong><b style="box-sizing:inherit;font-weight:700;">）<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [16].png" type="image/png" data-filename="equation.png" alt="f" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="10"/></b>，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用<b style="box-sizing:inherit;font-weight:700;">sigmoid函数<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [17].png" type="image/png" data-filename="equation.png" alt="\sigma" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="10"/></b>，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。</p><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div><img src="翻译4.1.1：神经网络笔记1（上）_files/d0cbce2f2654b8e70fe201fec2982c7d_b.png" type="image/png" data-filename="d0cbce2f2654b8e70fe201fec2982c7d_b.png" height="190" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="1598"/>左边是生物神经元，右边是数学模型。</div><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div>一个神经元前向传播的实例代码如下：</div><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">class</span> <span style="box-sizing:inherit;font-weight:bold;color:rgb(68, 85, 136);">Neuron</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 128, 128);">object</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># ...</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">def</span> <span style="box-sizing:inherit;font-weight:bold;color:rgb(153, 0, 0);">forward</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">inputs</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot; 假设输入和权重是1-D的numpy数组，偏差是一个数字 &quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">cell_body_sum</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">sum</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">inputs</span> <span style="box-sizing:inherit;font-weight:bold;">*</span> <span style="box-sizing:inherit;color:rgb(153, 153, 153);">self</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">weights</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-weight:bold;">+</span> <span style="box-sizing:inherit;color:rgb(153, 153, 153);">self</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">bias</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">firing_rate</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">1.0</span> <span style="box-sizing:inherit;font-weight:bold;">/</span> <span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">1.0</span> <span style="box-sizing:inherit;font-weight:bold;">+</span> <span style="box-sizing:inherit;">math</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">exp</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;font-weight:bold;">-</span><span style="box-sizing:inherit;">cell_body_sum</span><span style="box-sizing:inherit;">))</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># sigmoid激活函数</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">return</span> <span style="box-sizing:inherit;">firing_rate</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;">换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [18].png" type="image/png" data-filename="equation.png" alt="\sigma(x)=1/(1+e^{-x})" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="134"/>。在本节的末尾部分将介绍不同激活函数的细节。</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">粗糙模型</strong>：要注意这个对于生物神经元的建模是非常粗糙的：在实际中，有很多不同类型的神经元，每种都有不同的属性。生物神经元的树突可以进行复杂的非线性计算。突触并不就是一个简单的权重，它们是复杂的非线性动态系统。很多系统中，输出的峰值信号的精确时间点非常重要，说明速率编码的近似是不够全面的。鉴于所有这些已经介绍和更多未介绍的简化，如果你画出人类大脑和神经网络之间的类比，有神经科学背景的人对你的板书起哄也是非常自然的。如果你对此感兴趣，可以看看这份<a href="http://link.zhihu.com/?target=https%3A//physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">评论</a>或者最新的<a href="http://link.zhihu.com/?target=http%3A//www.sciencedirect.com/science/article/pii/S0959438814000130" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">另一份</a>。</p><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">作为线性分类器的单个神经元</h2><p style="box-sizing:inherit;margin:20px 0px;">神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力“喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">二分类Softmax分类器</b>。举例来说，可以把<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [19].png" type="image/png" data-filename="equation.png" alt="\displaystyle\sigma(\Sigma_iw_ix_i+b)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="94"/>看做其中一个分类的概率<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [20].png" type="image/png" data-filename="equation.png" alt="P(y_i=1|x_i;w)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="104"/>，其他分类的概率为<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [21].png" type="image/png" data-filename="equation.png" alt="P(y_i=0|x_i;w)=1-P(y_i=1|x_i;w)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="260"/>，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。</div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">二分类SVM分类器</b>。或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。</div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">理解正则化</b>。在SVM/Softmax的例子中，正则化损失从生物学角度可以看做<i style="box-sizing:inherit;">逐渐遗忘</i>，因为它的效果是让所有突触权重<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [22].png" type="image/png" data-filename="equation.png" alt="w" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="12"/>在参数更新过程中逐渐向着0变化。</div><blockquote style="box-sizing:inherit;padding-left:1.2em;margin:20px 0px;color:rgb(51, 51, 51);border-left:4px solid rgb(226, 227, 228);"><div>一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。</div></blockquote><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">常用激活函数</h2><p style="box-sizing:inherit;margin:20px 0px;"></p><div>每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数：</div><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div><img src="翻译4.1.1：神经网络笔记1（上）_files/677187e96671a4cac9c95352743b3806_b.png" type="image/png" data-filename="677187e96671a4cac9c95352743b3806_b.png" height="217" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="1350"/>左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。</div><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">Sigmoid。</b>sigmoid非线性函数的数学公式是<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [23].png" type="image/png" data-filename="equation.png" alt="\displaystyle\sigma(x)=1/(1+e^{-x})" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="134"/>，函数图像如上图的左边所示。在前一节中已经提到过，它输入实数值并将其“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（<b style="box-sizing:inherit;font-weight:700;">saturated</b>）的激活（1）。然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：</p><ul style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;"><i style="box-sizing:inherit;">Sigmoid函数饱和使梯度消失</i>。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;"><i style="box-sizing:inherit;">Sigmoid函数的输出不是零中心的</i>。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [24].png" type="image/png" data-filename="equation.png" alt="f=w^Tx+b" height="17" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="89"/>中每个元素都<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [25].png" type="image/png" data-filename="equation.png" alt="x>0" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="39"/>），那么关于<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [26].png" type="image/png" data-filename="equation.png" alt="w" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="12"/>的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [27].png" type="image/png" data-filename="equation.png" alt="f" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="10"/>而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。</li></ul><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">Tanh。</b>tanh非线性函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，<i style="box-sizing:inherit;">tanh非线性函数比sigmoid非线性函数更受欢迎</i>。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [28].png" type="image/png" data-filename="equation.png" alt="tanh(x)=2\sigma(2x)-1" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="152"/>。</div><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"><img src="翻译4.1.1：神经网络笔记1（上）_files/83682a138f6224230f5b0292d9c01bd2_b.png" type="image/png" data-filename="83682a138f6224230f5b0292d9c01bd2_b.png" height="251" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="1420"/>左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [29].png" type="image/png" data-filename="equation.png" alt="x=0" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="39"/>时函数值为0。当<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [30].png" type="image/png" data-filename="equation.png" alt="x>0" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="39"/>函数的斜率为1。右边是从 <a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">Krizhevsky</a>等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。</p><p style="box-sizing:inherit;margin:20px 0px;">————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">ReLU。</b>在近些年ReLU变得非常流行。它的函数公式是<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [31].png" type="image/png" data-filename="equation.png" alt="f(x)=max(0,x)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="121"/>。换句话说，这个激活函数就是一个关于0的阈值（如上图左侧）。使用ReLU有以下一些优缺点：</p><ul style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（ <a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">Krizhevsky</a>等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。</li></ul><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">Leaky ReLU。</b>Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x&lt;0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。所以其函数公式为<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [32].png" type="image/png" data-filename="equation.png" alt="f(x)=1(x<0)(\alpha x)+1(x>=0)(x)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="259"/>其中<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [33].png" type="image/png" data-filename="equation.png" alt="\alpha" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="10"/>是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。Kaiming He等人在2015年发布的论文<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.01852" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">Delving Deep into Rectifiers</a>中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。</p><p style="box-sizing:inherit;margin:20px 0px;"><b style="box-sizing:inherit;font-weight:700;">Maxout。</b>一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [34].png" type="image/png" data-filename="equation.png" alt="f(w^Tx+b)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="78"/>函数形式。一个相关的流行选择是Maxout（最近由<a href="http://link.zhihu.com/?target=http%3A//www-etud.iro.umontreal.ca/%7Egoodfeli/maxout.html" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">Goodfellow</a>等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [35].png" type="image/png" data-filename="equation.png" alt="max(w^T_1x+b_1,w^T_2x+b_2)" height="19" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="178"/>。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当<img src="翻译4.1.1：神经网络笔记1（上）_files/equation [36].png" type="image/png" data-filename="equation.png" alt="w_1,b_1=0" height="16" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="69"/>的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div>以上就是一些常用的神经元及其激活函数。最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。</div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">一句话</b>：“<i style="box-sizing:inherit;">那么该用那种呢？</i>”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。</div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">神经网络笔记1（上）完。</b></div></div></div></div></div></div></div></div></div><div><br/></div></div><div><br/></div><div><br/></div></span>
</div></body></html> 