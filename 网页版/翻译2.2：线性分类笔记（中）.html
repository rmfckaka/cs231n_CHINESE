<html>
<head>
  <title>翻译2.2：线性分类笔记（中）</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/305512 (zh-CN, DDL); Windows/10.0.15063 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1490"/>
<h1>翻译2.2：线性分类笔记（中）</h1>

<div>
<span><div style="-evernote-webclip:true"><div><span><span style="font-family: inherit; font-size: 24px; font-style: inherit; font-variant: inherit; font-weight: 700; line-height: inherit; color: rgb(51, 51, 51);">原文如下</span><br/></span></div><div style="font-size: 16px; display: inline-block;"><div style="box-sizing:border-box;"><div style="box-sizing:inherit;overflow-x:hidden;font-family:-apple-system, &quot;Helvetica Neue&quot;, Arial, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, &quot;WenQuanYi Micro Hei&quot;, sans-serif;font-weight:400;font-style:normal;text-rendering:optimizeLegibility;line-height:1;color:rgb(51, 51, 51);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;background:rgb(255, 255, 255);"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;"><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;word-break:break-word;white-space:normal;margin:30px 0px;line-height:1.7;"><p style="box-sizing:inherit;margin:20px 0px;">内容列表：</p><ul style="box-sizing:inherit;padding:0px;margin:20px 0px;padding-left:40px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">线性分类器简介</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">线性评分函数</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">阐明线性分类器</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">损失函数</li><ul style="box-sizing:inherit;padding:0px;padding-left:40px;margin:0px;"><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">多类SVM <i style="box-sizing:inherit;"><b style="box-sizing:inherit;font-weight:700;">译者注：中篇翻译截止处</b></i></li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">Softmax分类器</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">SVM和Softmax的比较</li></ul><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">基于Web的可交互线性分类器原型</li><li style="box-sizing:inherit;margin-top:10px;list-style-type:initial;list-style-position:outside;">小结</li></ul><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">损失函数 Loss function</h2><p style="box-sizing:inherit;margin:20px 0px;">在上一节定义了从图像像素值到所属类别的评分函数（score function），该函数的参数是权重矩阵<img src="翻译2.2：线性分类笔记（中）_files/equation.png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>。在函数中，数据<img src="翻译2.2：线性分类笔记（中）_files/equation [1].png" type="image/png" data-filename="equation.png" alt="(x_i,y_i)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="45"/>是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p><p style="box-sizing:inherit;margin:20px 0px;">回到之前那张猫的图像分类例子，它有针对“猫”，“狗”，“船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用<b style="box-sizing:inherit;font-weight:700;">损失函数（</b><strong style="box-sizing:inherit;">Loss Function）</strong>（有时也叫<b style="box-sizing:inherit;font-weight:700;">代价函数</b><strong style="box-sizing:inherit;">Cost Function</strong>或<b style="box-sizing:inherit;font-weight:700;">目标函数</b><strong style="box-sizing:inherit;">Objective</strong>）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。</p><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">多类支持向量机损失 Multiclass Support Vector Machine Loss</h2><p style="box-sizing:inherit;margin:20px 0px;">损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值<img src="翻译2.2：线性分类笔记（中）_files/equation [2].png" type="image/png" data-filename="equation.png" alt="\Delta" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。</p><p style="box-sizing:inherit;margin:20px 0px;">让我们更精确一些。回忆一下，第i个数据中包含图像<img src="翻译2.2：线性分类笔记（中）_files/equation [3].png" type="image/png" data-filename="equation.png" alt="x_i" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>的像素和代表正确类别的标签<img src="翻译2.2：线性分类笔记（中）_files/equation [4].png" type="image/png" data-filename="equation.png" alt="y_i" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="12"/>。评分函数输入像素数据，然后通过公式<img src="翻译2.2：线性分类笔记（中）_files/equation [5].png" type="image/png" data-filename="equation.png" alt="f(x_i,W)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="60"/>来计算不同分类类别的分值。这里我们将分值简写为<img src="翻译2.2：线性分类笔记（中）_files/equation [6].png" type="image/png" data-filename="equation.png" alt="s" height="7" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="7"/>。比如，针对第j个类别的得分就是第j个元素：<img src="翻译2.2：线性分类笔记（中）_files/equation [7].png" type="image/png" data-filename="equation.png" alt="s_j=f(x_i,W)_j" height="19" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="102"/>。针对第i个数据的多类SVM的损失函数定义如下：</p><div><img src="翻译2.2：线性分类笔记（中）_files/equation [8].png" type="image/png" data-filename="equation.png" alt="\displaystyle L_i=\sum_{j\not=y_i}max(0,s_j-s_{y_i}+\Delta)" height="38" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="210"/></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">举例</b>：用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值<img src="翻译2.2：线性分类笔记（中）_files/equation [9].png" type="image/png" data-filename="equation.png" alt="s=[13,-7,11]" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="105"/>。其中第一个类别是正确类别，即<img src="翻译2.2：线性分类笔记（中）_files/equation [10].png" type="image/png" data-filename="equation.png" alt="y_i=0" height="15" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="43"/>。同时假设<img src="翻译2.2：线性分类笔记（中）_files/equation [11].png" type="image/png" data-filename="equation.png" alt="\Delta" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（<img src="翻译2.2：线性分类笔记（中）_files/equation [12].png" type="image/png" data-filename="equation.png" alt="j\not=y_i" height="16" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="42"/>）加起来，所以我们得到两个部分：</div><div><img src="翻译2.2：线性分类笔记（中）_files/equation [13].png" type="image/png" data-filename="equation.png" alt="\displaystyle Li=max(0,-7-13+10)+max(0,11-13+10)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="360"/></div><p style="box-sizing:inherit;margin:20px 0px;">可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过<img src="翻译2.2：线性分类笔记（中）_files/equation [14].png" type="image/png" data-filename="equation.png" alt="max(0,-)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="72"/>函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别<img src="翻译2.2：线性分类笔记（中）_files/equation [15].png" type="image/png" data-filename="equation.png" alt="y_i" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="12"/>的分数比不正确类别分数高，而且至少要高<img src="翻译2.2：线性分类笔记（中）_files/equation [16].png" type="image/png" data-filename="equation.png" alt="\Delta" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>。如果不满足这点，就开始计算损失值。</p><p style="box-sizing:inherit;margin:20px 0px;">那么在这次的模型中，我们面对的是线性评分函数（<img src="翻译2.2：线性分类笔记（中）_files/equation [17].png" type="image/png" data-filename="equation.png" alt="f(x_i,W)=Wx_i" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="114"/>），所以我们可以将损失函数的公式稍微改写一下：</p><img src="翻译2.2：线性分类笔记（中）_files/equation [18].png" type="image/png" data-filename="equation.png" alt="\displaystyle L_i=\sum_{j\not=y_i}max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)" height="38" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="250"/><p style="box-sizing:inherit;margin:20px 0px;">其中<img src="翻译2.2：线性分类笔记（中）_files/equation [19].png" type="image/png" data-filename="equation.png" alt="w_j" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="17"/>是权重<img src="翻译2.2：线性分类笔记（中）_files/equation [20].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>的第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数<img src="翻译2.2：线性分类笔记（中）_files/equation [21].png" type="image/png" data-filename="equation.png" alt="f" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="10"/>公式，这样做就不是必须的了。</p><p style="box-sizing:inherit;margin:20px 0px;">在结束这一小节前，还必须提一下的属于是关于0的阀值：<img src="翻译2.2：线性分类笔记（中）_files/equation [22].png" type="image/png" data-filename="equation.png" alt="max(0,-)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="72"/>函数，它常被称为<b style="box-sizing:inherit;font-weight:700;">折叶损失（hinge loss）</b>。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是<img src="翻译2.2：线性分类笔记（中）_files/equation [23].png" type="image/png" data-filename="equation.png" alt="max(0,-)^2" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="79"/>，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。</p><blockquote style="box-sizing:inherit;padding-left:1.2em;margin:20px 0px;color:rgb(51, 51, 51);border-left:4px solid rgb(226, 227, 228);"><div>我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。</div></blockquote><p style="box-sizing:inherit;margin:20px 0px;">—————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div><img src="翻译2.2：线性分类笔记（中）_files/f254bd8d072128f1088c8cc47c3dff58_b.jpg" type="image/jpeg" data-filename="f254bd8d072128f1088c8cc47c3dff58_b.jpg" height="78" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;display:block;max-width:100%;margin-top:0.72em;margin-bottom:0.72em;cursor:-webkit-zoom-in;" width="647"/>多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。</div><p style="box-sizing:inherit;margin:20px 0px;">—————————————————————————————————————————</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">正则化（Regularization）：</strong>上面损失函数有一个问题。假设有一个数据集和一个权重集<b style="box-sizing:inherit;font-weight:700;">W</b>能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有<img src="翻译2.2：线性分类笔记（中）_files/equation [24].png" type="image/png" data-filename="equation.png" alt="L_i=0" height="15" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="45"/>）。问题在于这个<b style="box-sizing:inherit;font-weight:700;">W</b>并不唯一：可能有很多相似的<b style="box-sizing:inherit;font-weight:700;">W</b>都能正确地分类所有的数据。一个简单的例子：如果<b style="box-sizing:inherit;font-weight:700;">W</b>能够正确分类所有数据，即对于每个数据，损失值都是0。那么当<img src="翻译2.2：线性分类笔记（中）_files/equation [25].png" type="image/png" data-filename="equation.png" alt="\lambda>1" height="13" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="38"/>时，任何数乘<img src="翻译2.2：线性分类笔记（中）_files/equation [26].png" type="image/png" data-filename="equation.png" alt="\lambda W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="27"/>都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对<b style="box-sizing:inherit;font-weight:700;">W</b>乘以2将使得差距变成30。</p><p style="box-sizing:inherit;margin:20px 0px;">换句话说，我们希望能向某些特定的权重<b style="box-sizing:inherit;font-weight:700;">W</b>添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个<b style="box-sizing:inherit;font-weight:700;">正则化惩罚（regularization penalty）</b><img src="翻译2.2：线性分类笔记（中）_files/equation [27].png" type="image/png" data-filename="equation.png" alt="R(W)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="42"/>部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：</p><div><img src="翻译2.2：线性分类笔记（中）_files/equation [28].png" type="image/png" data-filename="equation.png" alt="R(W)=\sum_k \sum_l W^2_{k,l}" height="35" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="146"/></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>上面的表达式中，将<img src="翻译2.2：线性分类笔记（中）_files/equation [29].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：<strong style="box-sizing:inherit;">数据损失（data loss）</strong>，即所有样例的的平均损失<img src="翻译2.2：线性分类笔记（中）_files/equation [30].png" type="image/png" data-filename="equation.png" alt="L_i" height="14" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="15"/>，以及<strong style="box-sizing:inherit;">正则化损失（regularization loss）</strong>。完整公式如下所示：</div><div><img src="翻译2.2：线性分类笔记（中）_files/equation [31].png" type="image/png" data-filename="equation.png" alt="L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}_{data \ loss}+\underbrace{\lambda R(W)}_{regularization \ loss}" height="65" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="216"/></div><p style="box-sizing:inherit;margin:20px 0px;">将其展开完整公式是：</p><div><img src="翻译2.2：线性分类笔记（中）_files/equation [32].png" type="image/png" data-filename="equation.png" alt="L=\frac{1}{N}\sum_i\sum_{j\not=y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta)]+\lambda \sum_k \sum_l W^2_{k,l}" height="46" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="481"/></div><p style="box-sizing:inherit;margin:20px 0px;">其中，<img src="翻译2.2：线性分类笔记（中）_files/equation [33].png" type="image/png" data-filename="equation.png" alt="N" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="15"/>是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数<img src="翻译2.2：线性分类笔记（中）_files/equation [34].png" type="image/png" data-filename="equation.png" alt="\lambda" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</p><p style="box-sizing:inherit;margin:20px 0px;">除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了L2惩罚后，SVM们就有了<b style="box-sizing:inherit;font-weight:700;">最大边界（</b><strong style="box-sizing:inherit;">max margin）</strong>这一良好性质。（如果感兴趣，可以查看<a href="http://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes3.pdf" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">CS229课程</a>）。</p><p style="box-sizing:inherit;margin:20px 0px;">其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量<img src="翻译2.2：线性分类笔记（中）_files/equation [35].png" type="image/png" data-filename="equation.png" alt="x=[1,1,1,1]" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="93"/>，两个权重向量<img src="翻译2.2：线性分类笔记（中）_files/equation [36].png" type="image/png" data-filename="equation.png" alt="w_1=[1,0,0,0]" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="102"/>，<img src="翻译2.2：线性分类笔记（中）_files/equation [37].png" type="image/png" data-filename="equation.png" alt="w_2=[0.25,0.25,0.25,0.25]" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="185"/>。那么<img src="翻译2.2：线性分类笔记（中）_files/equation [38].png" type="image/png" data-filename="equation.png" alt="w^T_1x=w^T_2=1" height="19" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="102"/>，两个权重向量都得到同样的内积，但是<img src="翻译2.2：线性分类笔记（中）_files/equation [39].png" type="image/png" data-filename="equation.png" alt="w_1" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="17"/>的L2惩罚是1.0，而<img src="翻译2.2：线性分类笔记（中）_files/equation [40].png" type="image/png" data-filename="equation.png" alt="w_2" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>的L2惩罚是0.25。因此，根据L2惩罚来看，<img src="翻译2.2：线性分类笔记（中）_files/equation [41].png" type="image/png" data-filename="equation.png" alt="w_2" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>更好，因为它的正则化损失更小。从直观上来看，这是因为<img src="翻译2.2：线性分类笔记（中）_files/equation [42].png" type="image/png" data-filename="equation.png" alt="w_2" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免<i style="box-sizing:inherit;">过拟合</i>。</p><p style="box-sizing:inherit;margin:20px 0px;">需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重<img src="翻译2.2：线性分类笔记（中）_files/equation [43].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>正则化，而不正则化偏差<img src="翻译2.2：线性分类笔记（中）_files/equation [44].png" type="image/png" data-filename="equation.png" alt="b" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="7"/>。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当<img src="翻译2.2：线性分类笔记（中）_files/equation [45].png" type="image/png" data-filename="equation.png" alt="W=0" height="12" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="47"/>的特殊情况下，才能得到损失值为0。</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">代码</strong>：下面是一个无正则化部分的损失函数的Python实现，有非向量化和半向量化两个形式：</p><div style="box-sizing:inherit;flex-direction:column;align-items:stretch;flex-shrink:0;display:block;"><div style="box-sizing:inherit;margin:1em 0px;padding-right:1em;padding-left:1em;overflow:auto;font-family:Menlo, Monaco, Consolas, &quot;Andale Mono&quot;, &quot;lucida console&quot;, &quot;Courier New&quot;, monospace;font-size:14px;word-wrap:break-word;background:rgb(235, 238, 245);border-radius:4px;"><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">def</span> <span style="box-sizing:inherit;font-weight:bold;color:rgb(153, 0, 0);">L_i</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">y</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">unvectorized version. Compute the multiclass svm loss for a single example (x,y)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">with an appended bias dimension in the 3073-rd position (i.e. bias trick)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">delta</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">1.0</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># see notes about delta later in this section</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">scores</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">dot</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">)</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># scores becomes of size 10 x 1, the scores for each class</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">correct_class_score</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">scores</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">y</span><span style="box-sizing:inherit;">]</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">D</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">shape</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">0</span><span style="box-sizing:inherit;">]</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># number of classes, e.g. 10</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">loss_i</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0.0</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">for</span> <span style="box-sizing:inherit;">j</span> <span style="box-sizing:inherit;font-weight:bold;">in</span> <span style="box-sizing:inherit;color:rgb(0, 128, 128);">xrange</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">D</span><span style="box-sizing:inherit;">):</span> <span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># iterate over all wrong classes</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">if</span> <span style="box-sizing:inherit;">j</span> <span style="box-sizing:inherit;font-weight:bold;">==</span> <span style="box-sizing:inherit;">y</span><span style="box-sizing:inherit;">:</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># skip for the true class to only loop over incorrect classes</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">continue</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># accumulate loss for the i-th example</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">loss_i</span> <span style="box-sizing:inherit;font-weight:bold;">+=</span> <span style="box-sizing:inherit;color:rgb(0, 128, 128);">max</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">0</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">scores</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">j</span><span style="box-sizing:inherit;">]</span> <span style="box-sizing:inherit;font-weight:bold;">-</span> <span style="box-sizing:inherit;">correct_class_score</span> <span style="box-sizing:inherit;font-weight:bold;">+</span> <span style="box-sizing:inherit;">delta</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">return</span> <span style="box-sizing:inherit;">loss_i</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">def</span> <span style="box-sizing:inherit;font-weight:bold;color:rgb(153, 0, 0);">L_i_vectorized</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">y</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">A faster half-vectorized implementation. half-vectorized</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">refers to the fact that for a single example the implementation contains</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">no for loops, but there is still one loop over the examples (outside this function)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">delta</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">1.0</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">scores</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">dot</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">x</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># compute the margins for all classes in one vector operation</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">margins</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">maximum</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;color:rgb(0, 153, 153);">0</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">scores</span> <span style="box-sizing:inherit;font-weight:bold;">-</span> <span style="box-sizing:inherit;">scores</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">y</span><span style="box-sizing:inherit;">]</span> <span style="box-sizing:inherit;font-weight:bold;">+</span> <span style="box-sizing:inherit;">delta</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># on y-th position scores[y] - scores[y] canceled and gave delta. We want</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># to ignore the y-th position and only consider margin on max wrong class</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">margins</span><span style="box-sizing:inherit;">[</span><span style="box-sizing:inherit;">y</span><span style="box-sizing:inherit;">]</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;color:rgb(0, 153, 153);">0</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;">loss_i</span> <span style="box-sizing:inherit;font-weight:bold;">=</span> <span style="box-sizing:inherit;">np</span><span style="box-sizing:inherit;font-weight:bold;">.</span><span style="box-sizing:inherit;">sum</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">margins</span><span style="box-sizing:inherit;">)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">return</span> <span style="box-sizing:inherit;">loss_i</span></code></div><div><br/></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-weight:bold;">def</span> <span style="box-sizing:inherit;font-weight:bold;color:rgb(153, 0, 0);">L</span><span style="box-sizing:inherit;">(</span><span style="box-sizing:inherit;">X</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">y</span><span style="box-sizing:inherit;">,</span> <span style="box-sizing:inherit;">W</span><span style="box-sizing:inherit;">):</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">fully-vectorized implementation :</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- y is array of integers specifying correct class (e.g. 50,000-D array)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">- W are weights (e.g. 10 x 3073)</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;color:rgb(221, 51, 34);">&quot;&quot;&quot;</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># evaluate loss over all examples in X without using any for loops</span></code></div><div><code style="box-sizing:inherit;"><span style="box-sizing:inherit;font-style:italic;color:rgb(153, 153, 136);"># left as exercise to reader in the assignment</span></code></div></div></div><p style="box-sizing:inherit;margin:20px 0px;"></p><div>在本小节的学习中，一定要记得SVM损失采取了一种特殊的方法，使得能够衡量对于训练数据预测分类和实际分类标签的一致性。还有，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。</div><blockquote style="box-sizing:inherit;padding-left:1.2em;margin:20px 0px;color:rgb(51, 51, 51);border-left:4px solid rgb(226, 227, 228);"><div>接下来要做的，就是找到能够使损失值最小化的权重了。</div></blockquote><h2 style="box-sizing:inherit;font-style:inherit;font-variant:inherit;font-stretch:inherit;line-height:inherit;font-family:inherit;margin:20px 0px;font-size:24px;font-weight:700;">实际考虑</h2><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">设置Delta</strong>：你可能注意到上面的内容对超参数<img src="翻译2.2：线性分类笔记（中）_files/equation [46].png" type="image/png" data-filename="equation.png" alt="\Delta" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>及其设置是一笔带过，那么它应该被设置成什么值？需要通过交叉验证来求得吗？现在看来，该超参数在绝大多数情况下设为<img src="翻译2.2：线性分类笔记（中）_files/equation [47].png" type="image/png" data-filename="equation.png" alt="\Delta=1.0" height="13" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="56"/>都是安全的。超参数<img src="翻译2.2：线性分类笔记（中）_files/equation [48].png" type="image/png" data-filename="equation.png" alt="\Delta" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>和<img src="翻译2.2：线性分类笔记（中）_files/equation [49].png" type="image/png" data-filename="equation.png" alt="\lambda" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重<img src="翻译2.2：线性分类笔记（中）_files/equation [50].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将<img src="翻译2.2：线性分类笔记（中）_files/equation [51].png" type="image/png" data-filename="equation.png" alt="W" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="18"/>中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如<img src="翻译2.2：线性分类笔记（中）_files/equation [52].png" type="image/png" data-filename="equation.png" alt="\Delta=1" height="13" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="42"/>或<img src="翻译2.2：线性分类笔记（中）_files/equation [53].png" type="image/png" data-filename="equation.png" alt="\Delta=100" height="13" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="59"/>）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度<img src="翻译2.2：线性分类笔记（中）_files/equation [54].png" type="image/png" data-filename="equation.png" alt="\lambda" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>来控制）。</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">与二元支持向量机（</strong><strong style="box-sizing:inherit;">Binary Support Vector Machine</strong><strong style="box-sizing:inherit;">）的关系</strong>：在学习本课程前，你可能对于二元支持向量机有些经验，它对于第i个数据的损失计算公式是：</p><div><img src="翻译2.2：线性分类笔记（中）_files/equation [55].png" type="image/png" data-filename="equation.png" alt="\displaystyle L_i=Cmax(0,1-y_iw^Tx_i)+R(W)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="248"/></div><p style="box-sizing:inherit;margin:20px 0px;">其中，<img src="翻译2.2：线性分类笔记（中）_files/equation [56].png" type="image/png" data-filename="equation.png" alt="C" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>是一个超参数，并且<img src="翻译2.2：线性分类笔记（中）_files/equation [57].png" type="image/png" data-filename="equation.png" alt="y_i\in\{-1,1\}" height="17" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="85"/>。可以认为本章节介绍的SVM公式包含了上述公式，上述公式是多类支持向量机公式只有两个分类类别的特例。也就是说，如果我们要分类的类别只有两个，那么公式就化为二元SVM公式。这个公式中的<img src="翻译2.2：线性分类笔记（中）_files/equation [58].png" type="image/png" data-filename="equation.png" alt="C" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="13"/>和多类SVM公式中的<img src="翻译2.2：线性分类笔记（中）_files/equation [59].png" type="image/png" data-filename="equation.png" alt="\lambda" height="11" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="9"/>都控制着同样的权衡，而且它们之间的关系是<img src="翻译2.2：线性分类笔记（中）_files/equation [60].png" type="image/png" data-filename="equation.png" alt="C\propto\frac{1}{\lambda}" height="35" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="46"/></p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">备注：在初始形式中进行最优化</strong>。如果在本课程之前学习过SVM，那么对kernels，duals，SMO算法等将有所耳闻。在本课程（主要是神经网络相关）中，损失函数的最优化的始终在非限制初始形式下进行。很多这些损失函数从技术上来说是不可微的（比如当<img src="翻译2.2：线性分类笔记（中）_files/equation [61].png" type="image/png" data-filename="equation.png" alt="x=y" height="10" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="39"/>时，<img src="翻译2.2：线性分类笔记（中）_files/equation [62].png" type="image/png" data-filename="equation.png" alt="max(x,y)" height="18" style="box-sizing:inherit;overflow:hidden;margin-left:auto;margin-right:auto;max-width:100%;vertical-align:middle;display:inline-block;margin:0px 3px;" width="68"/>函数就不可微分），但是在实际操作中并不存在问题，因为通常可以使用次梯度。</p><p style="box-sizing:inherit;margin:20px 0px;"><strong style="box-sizing:inherit;">备注：其他多类SVM公式</strong>。需要指出的是，本课中展示的多类SVM只是多种SVM公式中的一种。另一种常用的公式是<i style="box-sizing:inherit;">One-Vs-All</i>（OVA）SVM，它针对每个类和其他类训练一个独立的二元分类器。还有另一种更少用的叫做<i style="box-sizing:inherit;">All-Vs-All</i>（AVA）策略。我们的公式是按照<a href="http://link.zhihu.com/?target=https%3A//www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">Weston and Watkins 1999 (pdf)</a>版本，比OVA性能更强（在构建有一个多类数据集的情况下，这个版本可以在损失值上取到0，而OVA就不行。感兴趣的话在论文中查阅细节）。最后一个需要知道的公式是Structured SVM，它将正确分类的分类分值和非正确分类中的最高分值的边界最大化。理解这些公式的差异超出了本课程的范围。本课程笔记介绍的版本可以在实践中安全使用，而被论证为最简单的OVA策略在实践中看起来也能工作的同样出色（在 Rikin等人2004年的论文<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf" rel="nofollow noreferrer" style="box-sizing:inherit;word-break:break-all;color:rgb(34, 85, 153);text-decoration:none;cursor:pointer;border-bottom:0px;" target="_blank">In Defense of One-Vs-All Classification (pdf)</a>中可查）。</p><p style="box-sizing:inherit;margin:20px 0px;"></p><div><b style="box-sizing:inherit;font-weight:700;">线性分类笔记（中）完</b>。</div></div></div></div></div></div></div></div></div><div><br/></div></div><div><br/></div><div><br/></div></span>
</div></body></html> 